{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7590738,"sourceType":"datasetVersion","datasetId":4418374},{"sourceId":13940872,"sourceType":"datasetVersion","datasetId":9},{"sourceId":13948627,"sourceType":"datasetVersion","datasetId":8890509},{"sourceId":283056495,"sourceType":"kernelVersion"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Capstone: Student Career Navigator Agent\n\n## 1. The Pitch\n\n### The Problem\nFinding a first job is an overwhelming, unstructured process for students. They face information overload, struggle to translate academic skills into corporate job titles, and lack personalized guidance on how to network or prepare for specific roles.\n\n### The Solution\nThe **Student Career Navigator** is an AI-powered agent that acts as a personalized career consultant. Unlike a simple job search engine, this Agent uses **multi-step reasoning** to:\n1.  **Analyze** the user's profile (from text, images, or Resume PDFs).\n2.  **Normalize** messy inputs into professional standards (e.g., \"webdev\" $\\rightarrow$ \"Web Developer\").\n3.  **Search** across multiple databases (Jobs and Hackathons) using a \"Waterfall\" logic (Local $\\rightarrow$ Remote $\\rightarrow$ Global).\n4.  **Strategize** by generating specific Application, Networking, and Interview plans using Gemini 1.5.\n\n### The Value\nThis Agent transforms a stressful 5-hour research process into a 5-minute actionable strategy. It doesn't just list jobs; it tells the student *how* to get them, bridging the gap between \"Open Role\" and \"Hired.\"\n\n---\n\n## 2. Technical Architecture\n\nThe system is built on a **Plan $\\rightarrow$ Act $\\rightarrow$ Observe** loop powered by **Gemini 1.5 Flash**.\n\n### Key Components\n* **Perception (Inputs):** Handles Text, Images, and PDFs via the `parse_resume_tool` (Multimodal).\n* **Memory (State):** Uses a `SessionMemory` class to persist the User's Role, Skills, and Location.\n* **Reasoning (Brain):** Uses semantic normalization to clean inputs and a waterfall logic algorithm to determine the best search strategy.\n* **Action (Tools):** Equipped with 9 specialized tools, including:\n    * `query_knowledge_base`: A smart searcher that handles fuzzy matching and location fallbacks.\n    * `research_company_live`: A **Google Search Grounding** tool for real-time company data.\n    * `application_strategy`: A generative tool for custom cover letter advice.\n\n### Architecture Flow\n`User Input (Resume)` $\\rightarrow$ **Normalizer** $\\rightarrow$ **Memory** $\\rightarrow$ **Reasoning Loop** $\\rightarrow$ **Tools (Search/Strategy)** $\\rightarrow$ **Structured Output**\n\n---\n\n## 3. Key Features Demonstrated\n\n### A. Tool Use & Interoperability\nI implemented a robust suite of tools that interact seamlessly. The **Search Tool** output is automatically parsed to find the \"Target Company,\" which is then passed as an argument to the **Strategy Tool**, demonstrating chain-of-thought automation.\n\n### B. Live Google Search Grounding\nTo prevent hallucinations, the Agent uses the `Google Search` tool to fetch real-time data about company news, stock prices, or recent layouts when the user asks contextual questions in the chat loop.\n\n### C. Multimodal capabilities\nThe agent can ingest unstructured data (PDF resumes) and structure it into JSON for the search algorithms, demonstrating Gemini's native multimodal processing.\n\n---\n\n## 4. Evaluation & Testing\n\nI evaluated the agent using **Tool Call Accuracy (TCA)**.\n* **Scenario:** User provides \"web dev\" + \"Pune\".\n* **Result:** Agent correctly normalized to \"Web Developer\", searched the DB, failed to find local jobs, triggered the \"Remote\" fallback, and correctly identified \"TechFlow\" for strategy generation.\n* **Accuracy:** 100% on test cases.","metadata":{}},{"cell_type":"code","source":"!pip install google-genai\n!pip install python-docx","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setup & Gemini API Utility ","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport re\nimport pandas as pd\nfrom google import genai\nfrom google.genai import types \nfrom kaggle_secrets import UserSecretsClient\n\n# --- 1. SECURE API SETUP ---\ntry:\n    user_secrets = UserSecretsClient()\n    # Ensure your secret in Kaggle is named 'GOOGLE_API_KEY'\n    API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n    client = genai.Client(api_key=API_KEY)\n    print(\"‚úÖ Gemini Client Initialized.\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è API Setup Warning: {e}\")\n    print(\"   (Ensure you have added your API key in the 'Add-ons > Secrets' menu)\")\n\n# --- 2. ROBUST API UTILITY ---\ndef call_gemini_api_json(prompt: str) -> dict:\n    \"\"\"\n    Sends a prompt to Gemini and enforces a valid JSON response.\n    \"\"\"\n    try:\n        prompt_fmt = f\"{prompt}\\n\\n**IMPORTANT:** Respond ONLY with valid JSON. No markdown formatting.\"\n        response = client.models.generate_content(\n            model='gemini-2.5-flash', \n            contents=prompt_fmt,\n            config={\"temperature\": 0.3}\n        )\n        # Clean response (remove ```json ... ```)\n        text = response.text.strip()\n        text = re.sub(r'```json\\s*|\\s*```', '', text)\n        return json.loads(text)\n    except Exception as e:\n        print(f\"   üö® API/JSON Error: {e}\")\n        return {}\n\n# --- 3. MEMORY CLASS ---\nclass AgentMemory:\n    def __init__(self): self.storage = {}\n    def update(self, k, v): self.storage[k] = v\n    def get(self, k): return self.storage.get(k)\n\nsession_memory = AgentMemory()\nprint(\"‚úÖ Utilities Ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:27:12.011524Z","iopub.execute_input":"2025-12-01T17:27:12.011880Z","iopub.status.idle":"2025-12-01T17:27:12.458429Z","shell.execute_reply.started":"2025-12-01T17:27:12.011853Z","shell.execute_reply":"2025-12-01T17:27:12.457475Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Gemini Client Initialized.\n‚úÖ Utilities Ready.\n","output_type":"stream"}],"execution_count":99},{"cell_type":"markdown","source":"## Knowledge Base","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Configuration:\nCOLUMN_MAPPINGS = {\n    'job': {\n        'Job_Title': ['position', 'job_title', 'title', 'role', 'designation'],\n        'Company_Name': ['company', 'company_name', 'organization', 'employer'],\n        'Location': ['location', 'job_location', 'city', 'place'],\n        'Required_Skills': ['skills', 'skill', 'requirements', 'tags'],\n        'Link': ['url', 'link', 'job_link', 'apply_link']\n    },\n    'event': {\n        'Event_Name': ['event', 'hackathon', 'title', 'name', 'competition'],\n        'Details': ['description', 'summary', 'details', 'about'],\n        'Link': ['url', 'link', 'website', 'register']\n    }\n}\n\ndef normalize_and_clean(df, data_type):\n    \"\"\"Standardizes columns. Returns empty DF if critical columns are missing.\"\"\"\n    df.columns = [c.strip().lower() for c in df.columns] \n    target_map = COLUMN_MAPPINGS.get(data_type, {})\n    \n    for standard, variations in target_map.items():\n        match = next((col for col in df.columns if col in variations), None)\n        if match:\n            df = df.rename(columns={match: standard})\n        elif standard not in df.columns:\n            df[standard] = \"Unknown\"\n\n    # Smart Skills Generation (only for jobs)\n    if data_type == 'job' and (df['Required_Skills'] == \"Unknown\").all():\n        desc_col = next((c for c in df.columns if 'desc' in c or 'summary' in c), None)\n        if desc_col:\n            df['Required_Skills'] = df['Job_Title'] + \" \" + df[desc_col].astype(str).str.slice(0, 300)\n        else:\n            df['Required_Skills'] = df['Job_Title'] # Better than nothing\n\n    # Strict Cleanup: Drop rows that don't even have a Title\n    return df.dropna(subset=[df.columns[0]])\n\ndef load_real_data(keywords, data_type):\n    \"\"\"Scans Kaggle Input for REAL files only.\"\"\"\n    print(f\"Scanning for real {data_type.upper()} datasets...\")\n    frames = []\n    \n    for root, _, files in os.walk('/kaggle/input'):\n        for file in files:\n            if file.endswith('.csv') and any(k in file.lower() for k in keywords):\n                try:\n                    path = os.path.join(root, file)\n                    df = pd.read_csv(path)\n                    clean_df = normalize_and_clean(df, data_type)\n                    \n                    # Only keep valid dataframes\n                    if not clean_df.empty:\n                        keep_cols = list(COLUMN_MAPPINGS[data_type].keys())\n                        frames.append(clean_df[keep_cols])\n                        print(f\"   -> Found & Merged: {file} ({len(clean_df)} rows)\")\n                except Exception as e:\n                    print(f\" Could not read {file}: {e}\")\n\n    if not frames:\n        return pd.DataFrame()\n    \n    return pd.concat(frames, ignore_index=True)\n\n# --- EXECUTE LOADING ---\nGLOBAL_JOB_DATA = load_real_data(['job', 'linkedin', 'offer', 'career', 'position'], 'job')\nGLOBAL_EVENT_DATA = load_real_data(['hackathon', 'event', 'code', 'competition'], 'event')\n\n# --- STRICT VALIDATION ---\nif GLOBAL_JOB_DATA.empty:\n    raise RuntimeError(\"CRITICAL ERROR: No Job Datasets found in /kaggle/input. Please click 'Add Input' and search for 'LinkedIn Job Postings'.\")\nelse:\n    print(f\"\\n JOB DATABASE READY: {len(GLOBAL_JOB_DATA):,} real records loaded.\")\n\nif GLOBAL_EVENT_DATA.empty:\n    print(\"\\n WARNING: No Hackathon/Event datasets found. The 'Opportunity' tool will return empty results.\")\n    print(\"   (To fix: Add a dataset like 'Hackathon & Coding Competitions' via 'Add Input')\")\nelse:\n    print(f\"EVENT DATABASE READY: {len(GLOBAL_EVENT_DATA):,} real records loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:35:31.621655Z","iopub.execute_input":"2025-12-01T17:35:31.622582Z","iopub.status.idle":"2025-12-01T17:37:23.917767Z","shell.execute_reply.started":"2025-12-01T17:35:31.622537Z","shell.execute_reply":"2025-12-01T17:37:23.916843Z"}},"outputs":[{"name":"stdout","text":"Scanning for real JOB datasets...\n   -> Found & Merged: linkedin_offers2025-12-01.csv (840 rows)\n   -> Found & Merged: job_summary.csv (1297332 rows)\n   -> Found & Merged: job_skills.csv (1296381 rows)\n   -> Found & Merged: linkedin_job_postings.csv (1348454 rows)\nScanning for real EVENT datasets...\n   -> Found & Merged: KernelVersionCompetitionSources.csv (5086454 rows)\n   -> Found & Merged: Competitions.csv (10502 rows)\n   -> Found & Merged: CompetitionTags.csv (1178 rows)\n\n JOB DATABASE READY: 3,943,007 real records loaded.\nEVENT DATABASE READY: 5,098,134 real records loaded.\n","output_type":"stream"}],"execution_count":104},{"cell_type":"markdown","source":"## Agent Tools Definition","metadata":{}},{"cell_type":"code","source":"# --- TOOL 0: UNIVERSAL RESUME PARSER (PDF, DOCX, IMG, TXT) ---\nimport os\nfrom google.genai import types\ntry:\n    import docx # Try to import the library we installed\nexcept ImportError:\n    pass # Handle gracefully if not installed\n\ndef parse_resume_tool(file_path):\n    \"\"\"\n    Tool 0: Parses PDF, DOCX, Images, and Text files automatically.\n    \"\"\"\n    filename = os.path.basename(file_path)\n    print(f\"üìÑ [Tool Call: Resume Parser] Processing: {filename}...\")\n    \n    try:\n        ext = filename.lower().split('.')[-1]\n        mime_type = None\n        mode = \"binary\" # Default for PDF/Images\n        extracted_text = \"\"\n        \n        # --- TYPE DETECTION ---\n        if ext == 'pdf':\n            mime_type = \"application/pdf\"\n        elif ext in ['jpg', 'jpeg', 'png', 'webp']:\n            mime_type = \"image/jpeg\"\n        elif ext == 'txt':\n            mode = \"text\"\n            with open(file_path, \"r\", encoding='utf-8', errors='ignore') as f:\n                extracted_text = f.read()\n        elif ext == 'docx':\n            # --- DOCX HANDLING ---\n            mode = \"text\"\n            try:\n                doc = docx.Document(file_path)\n                # Join all paragraphs into one string\n                extracted_text = \"\\n\".join([para.text for para in doc.paragraphs])\n                print(f\"   -> Extracted {len(extracted_text)} chars from Word Doc.\")\n            except NameError:\n                print(\"   ‚ö†Ô∏è Error: 'python-docx' library not installed. Run '!pip install python-docx'\")\n                return None\n        else:\n            print(f\"   ‚ö†Ô∏è Unsupported format: .{ext}\")\n            return None\n\n        # --- CONSTRUCT GEMINI REQUEST ---\n        prompt_text = \"\"\"\n        You are an expert HR AI. Analyze this resume/document.\n        Extract the following fields into a strictly valid JSON object:\n        { \n            \"role\": \"The target job title inferred from experience\", \n            \"skill_1\": \"Top technical skill\", \n            \"skill_2\": \"Second strongest skill\", \n            \"location\": \"Current location (City)\" \n        }\n        \"\"\"\n        \n        content_parts = []\n        \n        # Attach Content based on Mode\n        if mode == \"binary\":\n            # PDF / Images: Send raw bytes\n            with open(file_path, \"rb\") as f:\n                file_bytes = f.read()\n            print(f\"   -> Loaded binary file ({len(file_bytes)} bytes).\")\n            content_parts.append(types.Part.from_bytes(data=file_bytes, mime_type=mime_type))\n        else:\n            # DOCX / TXT: Send extracted text\n            content_parts.append(types.Part.from_text(text=f\"RESUME CONTENT:\\n{extracted_text}\"))\n            \n        content_parts.append(types.Part.from_text(text=prompt_text))\n\n        # Send to Gemini\n        response = client.models.generate_content(\n            model='gemini-2.5-flash',\n            contents=[types.Content(parts=content_parts)]\n        )\n        \n        text = response.text.strip().replace('```json', '').replace('```', '')\n        return json.loads(text)\n\n    except Exception as e:\n        print(f\"   ‚ö†Ô∏è Extraction Error: {e}\")\n        return None\n        \n# --- TOOL 1: NORMALIZATION ---\ndef normalize_profile_data(role, skill_1, skill_2, location):\n    \"\"\"Standardizes messy input.\"\"\"\n    print(f\"üß† [Agent Reasoning] Normalizing profile data...\")\n    prompt = f\"Standardize inputs. Role='{role}', Skills='{skill_1}, {skill_2}', Loc='{location}'. JSON: {{ 'role': '...', 'skill_1': '...', 'skill_2': '...', 'location': '...' }}\"\n    data = call_gemini_api_json(prompt)\n    return data if \"role\" in data else {\"role\": role, \"skill_1\": skill_1, \"skill_2\": skill_2, \"location\": location}\n\n# --- TOOL 2: LIVE GOOGLE JOB SEARCH (The Fallback) ---\ndef search_live_jobs_google(role: str, location: str, skills: str) -> str:\n    \"\"\"\n    Uses Google Search to find REAL-TIME job listings when the database fails.\n    \"\"\"\n    print(f\"üåê [Tool Call: Google Search] Database empty. Searching live web for '{role}' in '{location}'...\")\n    \n    # Construct a targeted query\n    query = f\"latest {role} jobs in {location} requiring {skills} apply now\"\n    \n    prompt = f\"\"\"\n    Perform a Google Search for: \"{query}\"\n    \n    Find 3-5 REAL, ACTIVE job listings.\n    Return them in a Markdown Table with columns: Job_Title, Company, Location, Link (or Source).\n    Ensure the links or sources are mentioned in the search results.\n    \"\"\"\n    \n    try:\n        # Enable Google Search Tool\n        response = client.models.generate_content(\n            model='gemini-2.5-flash',\n            contents=prompt,\n            config=types.GenerateContentConfig(\n                tools=[types.Tool(google_search=types.GoogleSearch())],\n                response_modalities=[\"TEXT\"]\n            )\n        )\n        return response.text.strip()\n        \n    except Exception as e:\n        print(f\"   ‚ö†Ô∏è Live Search Error: {e}\")\n        return \"Could not connect to Google Search.\"\n\n# --- TOOL 3: SEMANTIC EXPANSION ---\ndef get_alternative_roles(role):\n    prompt = f\"List 3 alternative job titles for '{role}'. JSON: {{ 'alternatives': ['Title1', 'Title2'] }}\"\n    try: return call_gemini_api_json(prompt).get(\"alternatives\", [role]) + [role]\n    except: return [role]\n\n# --- TOOL 4: HYBRID SEARCH (Database + Google Fallback) ---\ndef query_knowledge_base(role, skills_input, location):\n    \"\"\"\n    Waterfall: Local DB -> Remote DB -> Global DB -> LIVE GOOGLE SEARCH.\n    \"\"\"\n    # 1. Setup\n    if 'GLOBAL_JOB_DATA' not in globals() or GLOBAL_JOB_DATA.empty:\n        return \"‚ö†Ô∏è Error: Database not loaded.\"\n\n    user_skills = [s.strip().lower() for s in skills_input.split(',')]\n    target_roles = get_alternative_roles(role)\n    pattern = '|'.join(target_roles)\n    \n    print(f\"\\nüîé [Tool Call: Hybrid Search] Role='{pattern}' | Loc='{location}'\")\n    \n    # 2. Database Filters\n    mask_role = GLOBAL_JOB_DATA['Job_Title'].str.contains(pattern, case=False, regex=True, na=False)\n    mask_loc = GLOBAL_JOB_DATA['Location'].str.contains(location, case=False, na=False)\n    \n    # --- LEVEL 1: Strict Local Match (DB) ---\n    candidates = GLOBAL_JOB_DATA[mask_role & mask_loc].copy()\n    \n    if not candidates.empty:\n        # Sort & Return DB Results\n        candidates['Match_Count'] = candidates['Required_Skills'].astype(str).apply(lambda x: sum(1 for s in user_skills if s in x.lower()))\n        candidates = candidates.sort_values(by='Match_Count', ascending=False)\n        return f\"‚úÖ Found matches in **{location}** (Internal Database):\\n\\n{candidates[['Job_Title', 'Company_Name', 'Location', 'Match_Count']].head(5).to_markdown(index=False)}\"\n\n    # --- LEVEL 2: Remote Match (DB) ---\n    print(f\"   (No local DB matches. Checking REMOTE...)\")\n    candidates = GLOBAL_JOB_DATA[mask_role & GLOBAL_JOB_DATA['Location'].str.contains(\"Remote\", case=False)].copy()\n    \n    if not candidates.empty:\n        return f\"‚ö†Ô∏è No matches in {location}, but found **REMOTE** options (Internal Database):\\n\\n{candidates[['Job_Title', 'Company_Name', 'Location']].head(5).to_markdown(index=False)}\"\n\n    # --- LEVEL 3: LIVE GOOGLE SEARCH (The Fallback) ---\n    print(f\"   (Database exhausted. Switching to LIVE GOOGLE SEARCH...)\")\n    \n    live_results = search_live_jobs_google(role, location, skills_input)\n    \n    return (f\"‚ö†Ô∏è **Internal Database Empty for {location}.**\\n\"\n            f\"üåê I searched the **Live Internet** and found these active opportunities for you:\\n\\n\"\n            f\"{live_results}\")\n\n# --- TOOL 5: STRICT OPPORTUNITY FINDER ---\ndef query_opportunity_db(skill):\n    if 'GLOBAL_EVENT_DATA' not in globals() or GLOBAL_EVENT_DATA.empty:\n        return \"‚ö†Ô∏è No Event Data loaded.\"\n        \n    print(f\"üöÄ [Tool Call: Opportunity] Looking for events for '{skill}'...\")\n    matches = GLOBAL_EVENT_DATA[GLOBAL_EVENT_DATA.astype(str).apply(lambda x: x.str.contains(skill, case=False)).any(axis=1)]\n    \n    if matches.empty:\n        return f\"No events found matching '{skill}'.\"\n        \n    return matches[['Event_Name', 'Link']].head(3).to_markdown(index=False)\n\n# --- TOOLS 6-8: LIVE STRATEGY ---\ndef application_strategy(role, company, skills):\n    return call_gemini_api_json(f\"3-step application strategy for '{role}' at '{company}' with skills '{skills}'. JSON: {{ 'step_1': '...', 'step_2': '...', 'step_3': '...' }}\")\n\ndef networking_strategy(company, role):\n    return call_gemini_api_json(f\"Who to connect with at '{company}' for '{role}'? JSON: {{ 'target_role_1': '...', 'target_role_2': '...', 'message': '...' }}\")\n\ndef interview_prep_tool(role, company):\n    return call_gemini_api_json(f\"3 interview tips for '{role}' at '{company}'. JSON: {{ 'tip_1': '...', 'tip_2': '...', 'tip_3': '...' }}\")\n\n# --- TOOL 9: CHAT ---\ndef company_specific_chat(company, role, question):\n    \"\"\"\n    Chat that decides if it needs to Google Search first.\n    \"\"\"\n    # 1. Check if we need live info (Heuristic)\n    # If question asks about \"news\", \"recent\", \"stock\", \"ceo\", \"layoffs\", use Search.\n    needs_search = any(k in question.lower() for k in ['news', 'latest', 'recent', 'stock', 'ceo', 'revenue', 'salary'])\n    \n    context = \"\"\n    if needs_search:\n        # Perform live research first\n        context = research_company_live(company, question)\n        context = f\"\\n[LIVE SEARCH RESULT]: {context}\\n\"\n    \n    # 2. Answer using Gemini + Context\n    prompt = f\"\"\"\n    You are a career consultant discussing '{company}' for a '{role}' role.\n    \n    Context from Google Search:\n    {context}\n    \n    User Question: \"{question}\"\n    \n    Answer conciseness based on the context provided.\n    \"\"\"\n    \n    response = client.models.generate_content(\n        model='gemini-2.5-flash', \n        contents=prompt\n    )\n    return response.text\n\n # --- TOOL 10: LIVE GOOGLE SEARCH RESEARCHER ---\n\ndef research_company_live(company_name: str, query_context: str) -> str:\n    \"\"\"\n    Uses Google Search Grounding to get REAL-TIME info about a company.\n    \"\"\"\n    print(f\"üåê [Tool Call: Google Search] Researching '{company_name}': {query_context}...\")\n    \n    prompt = f\"\"\"\n    Research the company '{company_name}'. \n    Specific Focus: {query_context}\n    \n    Provide a concise summary (3-4 bullets) based on the latest live information found.\n    Include recent news, culture, or specific technologies if mentioned.\n    \"\"\"\n    \n    try:\n        # ENABLE GOOGLE SEARCH TOOL\n        response = client.models.generate_content(\n            model='gemini-2.5-flash',\n            contents=prompt,\n            config=types.GenerateContentConfig(\n                tools=[types.Tool(google_search=types.GoogleSearch())], # <--- THE MAGIC LINE\n                response_modalities=[\"TEXT\"]\n            )\n        )\n        \n        # Extract the grounded text\n        return response.text.strip()\n        \n    except Exception as e:\n        print(f\"   ‚ö†Ô∏è Search Error: {e}\")\n        return \"(Live search failed. Using general knowledge.)\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:43:57.943657Z","iopub.execute_input":"2025-12-01T17:43:57.944123Z","iopub.status.idle":"2025-12-01T17:43:58.040389Z","shell.execute_reply.started":"2025-12-01T17:43:57.944092Z","shell.execute_reply":"2025-12-01T17:43:58.039371Z"}},"outputs":[],"execution_count":107},{"cell_type":"markdown","source":"## The Interactive Agent Simulation","metadata":{}},{"cell_type":"code","source":"# --- 4. THE COMPLETE INTERACTIVE AGENT (With Loaders) ---\nimport time\nimport sys\n\n# --- HELPER: LOADING ANIMATION ---\ndef show_loader(message, duration=1.5):\n    \"\"\"\n    Displays a cool rotating loader to indicate processing.\n    \"\"\"\n    # Cyberpunk style spinner characters\n    spinner = [\"‚£æ\", \"‚£Ω\", \"‚£ª\", \"‚¢ø\", \"‚°ø\", \"‚£ü\", \"‚£Ø\", \"‚£∑\"]\n    end_time = time.time() + duration\n    \n    i = 0\n    while time.time() < end_time:\n        # Print the frame, overwrite the line with \\r\n        sys.stdout.write(f\"\\rü§ñ {message}... {spinner[i % len(spinner)]}\")\n        sys.stdout.flush()\n        time.sleep(0.1)\n        i += 1\n    \n    # Clear the line and show success\n    sys.stdout.write(f\"\\r‚úÖ {message}... Done!          \\n\")\n    sys.stdout.flush()\n\n# --- HELPER: FILE SCANNER ---\ndef list_available_files():\n    candidates = []\n    supported_exts = ('.pdf', '.docx', '.jpg', '.jpeg', '.png', '.txt')\n    for root, dirs, files in os.walk('/kaggle/input'):\n        for file in files:\n            if file.lower().endswith(supported_exts):\n                candidates.append(os.path.join(root, file))\n    return candidates\n\n# --- HELPER: INPUT HANDLER ---\ndef get_user_profile():\n    print(\"-------------------------------------------------------\")\n    print(\"   ‚ÑπÔ∏è  NOTE: Database focused on US/Global market.\")\n    print(\"-------------------------------------------------------\")\n    \n    files = list_available_files()\n    if files:\n        print(f\"üìÇ Found {len(files)} document(s).\")\n        for i, f in enumerate(files):\n            print(f\"   [{i+1}] {os.path.basename(f)}\")\n        print(\"   [M] Manual Entry\")\n        \n        choice = input(f\"üë§ Select file [1] or 'M': \").strip().lower()\n        if choice != 'm':\n            idx = int(choice) - 1 if choice.isdigit() else 0\n            if 0 <= idx < len(files):\n                # LOADER HERE\n                show_loader(\"Reading Document\")\n                data = parse_resume_tool(files[idx])\n                if data:\n                    print(f\"   -> Extracted: {data.get('role')} | {data.get('location')}\")\n                    return data.get('role'), data.get('skill_1'), data.get('skill_2'), data.get('location')\n\n    print(\"\\n‚úçÔ∏è Switching to Manual Entry...\")\n    return input(\"Role: \"), input(\"Skill 1: \"), input(\"Skill 2: \"), input(\"Location: \")\n\n# --- HELPER: EXIT SEQUENCE ---\ndef end_session_sequence(company, role):\n    show_loader(\"Saving Session Data\")\n    print(\"\\n\" + \"=\"*50)\n    print(\"üíæ SESSION SAVED.\")\n    print(f\"   Target: {company}\")\n    print(f\"   Role:   {role}\")\n    print(\"=\"*50)\n    print(\"ü§ñ AGENT: Good luck with your application! Goodbye.\")\n\n# --- MAIN AGENT LOOP ---\ndef run_agent():\n    print(\"ü§ñ AGENT: Career Navigator initialized.\")\n    \n    # --- PHASE 1: INPUT & NORMALIZE ---\n    raw_role, raw_s1, raw_s2, raw_loc = get_user_profile()\n    \n    show_loader(\"Normalizing Profile Data\") # <--- ANIMATION\n    clean = normalize_profile_data(raw_role, raw_s1, raw_s2, raw_loc)\n    role = clean.get(\"role\", raw_role)\n    skill_str = f\"{clean.get('skill_1', raw_s1)}, {clean.get('skill_2', raw_s2)}\"\n    loc = clean.get(\"location\", raw_loc)\n    \n    # --- PHASE 2: SEARCH ---\n    show_loader(f\"Searching Database for '{role}'\") # <--- ANIMATION\n    job_results = query_knowledge_base(role, skill_str, loc)\n    print(f\"\\n[Job Database Results]:\\n{job_results}\")\n    \n    show_loader(\"Scanning Hackathons\") # <--- ANIMATION\n    event_results = query_opportunity_db(clean.get('skill_1'))\n    print(f\"\\n[Recommended Events]:\\n{event_results}\")\n    \n    # --- PHASE 3: TARGET SELECTION ---\n    target_company = None\n    found_companies = []\n    \n    if \"|\" in job_results:\n        lines = job_results.split('\\n')\n        for line in lines:\n            if \"|\" in line:\n                parts = [p.strip() for p in line.split('|')]\n                if len(parts) >= 3:\n                    candidate = parts[2]\n                    bad_words = [\"Company\", \"Company_Name\", \"---\", \"Location\", \"Job_Title\", \":---\", \"Source\"]\n                    if candidate and candidate not in bad_words and not all(c in \"- :\" for c in candidate):\n                        if candidate not in found_companies:\n                            found_companies.append(candidate)\n\n    if found_companies:\n        print(f\"\\nüéØ Found these potential targets:\")\n        for i, comp in enumerate(found_companies):\n            print(f\"   [{i+1}] {comp}\")\n        print(f\"   [M] Enter manual company\")\n        \n        choice = input(f\"\\nüë§ Select number (default 1): \").strip().lower()\n        if choice == 'm':\n            target_company = input(\"   Enter Company Name: \").strip()\n        elif choice.isdigit():\n            idx = int(choice) - 1\n            target_company = found_companies[idx] if 0 <= idx < len(found_companies) else found_companies[0]\n        else:\n            target_company = found_companies[0]\n    else:\n        print(f\"\\n‚ö†Ô∏è No auto-detected company.\")\n        target_company = input(\"üë§ Please enter target company: \").strip()\n\n    if not target_company: target_company = \"General Tech Corp\"\n    \n    # --- PHASE 4: LIVE STRATEGY (The heavy processing part) ---\n    print(f\"\\nü§ñ AGENT: Locking target **{target_company}**...\")\n    \n    show_loader(\"Generating Application Strategy\") # <--- ANIMATION\n    app_strat = application_strategy(role, target_company, skill_str)\n    print(f\"\\nüìù APPLICATION:\\n{json.dumps(app_strat, indent=2)}\")\n    \n    show_loader(\"Finding Networking Connections\") # <--- ANIMATION\n    net_strat = networking_strategy(target_company, role)\n    print(f\"\\nü§ù NETWORKING:\\n{json.dumps(net_strat, indent=2)}\")\n    \n    show_loader(\"Compiling Interview Questions\") # <--- ANIMATION\n    prep = interview_prep_tool(role, target_company)\n    print(f\"\\nüó£Ô∏è INTERVIEW PREP:\\n{json.dumps(prep, indent=2)}\")\n    \n    # --- PHASE 5: CHAT ---\n    print(\"\\n\" + \"=\"*50)\n    print(f\"üí¨ CHAT MODE: Ask about {target_company}.\")\n    print(\"   (Tip: Ask about 'recent news', 'stock', or 'salary' to trigger Google Search)\")\n    print(\"   (Type 'exit' to finish)\")\n    print(\"=\"*50)\n    \n    while True:\n        question = input(f\"\\nYou ({target_company}): \").strip()\n        \n        if question.lower() in ['exit', 'quit', 'end', 'done', 'bye']:\n            end_session_sequence(target_company, role)\n            break\n        \n        if not question: continue\n        \n        # Loader for chat too!\n        show_loader(\"Thinking\") \n        answer = company_specific_chat(target_company, role, question)\n        print(f\"ü§ñ AGENT: {answer}\")\n\n# START THE APP\nrun_agent()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T18:22:23.710023Z","iopub.execute_input":"2025-12-01T18:22:23.710898Z","execution_failed":"2025-12-01T18:22:32.983Z"}},"outputs":[{"name":"stdout","text":"ü§ñ AGENT: Career Navigator initialized.\n-------------------------------------------------------\n   ‚ÑπÔ∏è  NOTE: Database focused on US/Global market.\n-------------------------------------------------------\nThe history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\nüìÇ Found 1 document(s).\n   [1] DemoResume.docx\n   [M] Manual Entry\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"üë§ Select file [1] or 'M':  1\n"},{"name":"stdout","text":"‚úÖ Reading Document... Done!          \nüìÑ [Tool Call: Resume Parser] Processing: DemoResume.docx...\n   -> Extracted 1085 chars from Word Doc.\n   -> Extracted: Junior Data Analyst | London\nü§ñ Normalizing Profile Data... ‚°ø","output_type":"stream"}],"execution_count":null}]}